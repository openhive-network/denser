stages:
  - build
  - replay-and-sync
  - test
  - test-report-aggregate
  - deploy
  - publish
  - cleanup

variables:
  # Variables required by Common CI jobs
  CI_COMMON_JOB_VERSION: '682ba2e16cfc4ef192478d2d21c4b5a52bbd4678'
  DOCKER_BUILDER_TAG: '$CI_COMMON_JOB_VERSION'
  DOCKER_DIND_TAG: '$CI_COMMON_JOB_VERSION'
  IMAGE_REMOVER_TAG: '$CI_COMMON_JOB_VERSION'

  # Git settings
  GIT_DEPTH: 20
  GIT_SUBMODULE_STRATEGY: none
  GIT_STRATEGY: clone

  # Runner feature flags
  FF_ENABLE_JOB_CLEANUP: 1

  # HAF API node configuration
  API_NODE_VERSION: '90486532'
  HAF_COMMIT_SHA: '6fa48e73dcf06140424d8c70e610fa421313ac0d'
  HIVEMIND_INSTANCE_VERSION: 'daede252'
  HAFAH_VERSION: 'a7cf1e38'
  BLOCK_LOG_VERSION: '42bf3ac5'
  REPUTATION_TRACKER_VERSION: '762663b3'
  HIVEMIND_INSTANCE_IMAGE: registry.gitlab.syncad.com/hive/hivemind/instance
  HAFAH_REGISTRY: registry.gitlab.syncad.com/hive/hafah/instance
  REPUTATION_TRACKER_REGISTRY: registry.gitlab.syncad.com/hive/reputation_tracker
  # BLOCK_LOG_SOURCE_DIR: "/blockchain/block_log_mirror_44"
  REPLAY_DIRECTORY_PREFIX: '/cache/replay_data_denser_haf_api_node'
  REPLAY_DIRECTORY: '${REPLAY_DIRECTORY_PREFIX}_${API_NODE_VERSION}_${HAF_COMMIT_SHA}_${HIVEMIND_INSTANCE_VERSION}_${HAFAH_VERSION}_${HIVE_API_NODE_VERSION}_${BLOCK_LOG_VERSION}'
  REPLAY_PIPELINE_DIRECTORY: '${REPLAY_DIRECTORY_PREFIX}_${CI_PIPELINE_ID}'
  DOCKER_TLS_CERTDIR: '${REPLAY_PIPELINE_DIRECTORY}_certs'
  BLOCK_LOG_SOURCE: '${REPLAY_PIPELINE_DIRECTORY}_block_log'
  BLOCK_LOG_SOURCE_DIR: '${BLOCK_LOG_SOURCE}'

  # Other settings
  HAFAH_PROJECT_ID: 308
  HIVEMIND_PROJECT_ID: 213
  HAF_API_NODE_PROJECT_ID: 444
  REPUTATION_TRACKER_PROJECT_ID: 418
  CADDY_TAG: '2.7.6'
  PLAYWRIGHT_TAG: 'v1.46.0-jammy'
  PIPELINE_TRIGGER_TAG: '2.9.0'

include:
  - template: Workflows/Branch-Pipelines.gitlab-ci.yml
  - project: 'hive/common-ci-configuration'
    ref: 682ba2e16cfc4ef192478d2d21c4b5a52bbd4678 # develop
    file:
      - '/templates/docker_image_jobs.gitlab-ci.yml'
      - '/templates/cache_cleanup.gitlab-ci.yml'
  - project: 'hive/haf'
    ref: 6fa48e73dcf06140424d8c70e610fa421313ac0d #develop
    file: '/scripts/ci-helpers/prepare_data_image_job.yml'
  - project: 'hive/haf_api_node'
    ref: 90486532d2ade984e25952d9d13219097e5dc3b5
    file: 'ci/node-replay.gitlab-ci.yml'

.docker_build_template:
  extends: .docker_image_builder_job_template
  stage: build
  before_script:
    - !reference [.docker_image_builder_job_template, before_script]
    - |
      echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
      docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
      echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
  script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):tag[collapsed=true]\r\e[0KDetermining tag for the new image..."
      if [[ "$CI_COMMIT_BRANCH" == "$CI_DEFAULT_BRANCH" ]]; then
        echo "Running on default branch '$CI_DEFAULT_BRANCH': tag = 'latest'"
        export TAG="latest"
      else
        echo "Running on branch '$CI_COMMIT_BRANCH': tag = $CI_COMMIT_REF_SLUG"
        export TAG="$CI_COMMIT_REF_SLUG"
      fi
      echo -e "\e[0Ksection_end:$(date +%s):tag\r\e[0K"
      echo -e "\e[0Ksection_start:$(date +%s):build[collapsed=true]\r\e[0KBaking image "$CI_REGISTRY_IMAGE/${TURBO_APP_NAME}:${TAG}"..."
      git config --global --add safe.directory "$CI_PROJECT_DIR"
      scripts/build_instance.sh --progress=plain "$CI_PROJECT_DIR"
      echo "${TURBO_APP_NAME^^}_IMAGE_NAME=${CI_REGISTRY_IMAGE}/${TURBO_APP_NAME}:${CI_COMMIT_SHORT_SHA}" > "${TURBO_APP_NAME}-docker-build.env"
      echo "Unique image tag:"
      cat "${TURBO_APP_NAME}-docker-build.env"
      echo -e "\e[0Ksection_end:$(date +%s):build\r\e[0K"
  artifacts:
    paths:
      - ${TURBO_APP_NAME}-docker-build.env
    reports:
      dotenv: ${TURBO_APP_NAME}-docker-build.env
  tags:
    - public-runner-docker
    - hived-for-tests

# We need to creat temporary tags to be used with Trigger API since it doesn't work with commit SHAs.
# We also need to skip CI when pushing tags, since we want to run just the Docker build job and not the
# entire pipeline.
create_temporary_tags:
  extends: .job-defaults
  stage: build
  image: 'registry.gitlab.syncad.com/hive/hive/ci-base-image${TEST_IMAGE_TAG}'
  before_script:
    - |
      set -e
      git config --global user.email "${GITLAB_USER_EMAIL}"
      git config --global user.name "${GITLAB_USER_NAME}"
      git clone https://gitlab-ci-token:${HAFAH_ACCESS_TOKEN}@gitlab.syncad.com/hive/HAfAH.git hafah
      git clone https://gitlab-ci-token:${HIVEMIND_ACCESS_TOKEN}@gitlab.syncad.com/hive/hivemind.git hivemind
      git clone https://gitlab-ci-token:${HAF_API_NODE_ACCESS_TOKEN}@gitlab.syncad.com/hive/haf_api_node.git haf_api_node
      git clone https://gitlab-ci-token:${REPUTATION_TRACKER_ACCESS_TOKEN}@gitlab.syncad.com/hive/reputation_tracker.git reptracker
  script:
    - |
      set -e

      pushd hafah
      git tag ${CI_PIPELINE_ID} ${HAFAH_VERSION}
      git push origin ${CI_PIPELINE_ID} -o ci.skip
      popd

      pushd hivemind
      git tag ${CI_PIPELINE_ID} ${HIVEMIND_INSTANCE_VERSION}
      git push origin ${CI_PIPELINE_ID} -o ci.skip
      popd

      pushd haf_api_node
      git tag ${CI_PIPELINE_ID} ${API_NODE_VERSION}
      git push origin ${CI_PIPELINE_ID} -o ci.skip
      popd

      pushd reptracker
      git tag ${CI_PIPELINE_ID} ${REPUTATION_TRACKER_VERSION}
      git push origin ${CI_PIPELINE_ID} -o ci.skip
      popd
  tags:
    - public-runner-docker

# For some reason a regular trigger job fails to recognize temporary tags, so we have to use a workaround
.external-build:
  extends: .job-defaults
  stage: build
  image: registry.gitlab.com/finestructure/pipeline-trigger:${PIPELINE_TRIGGER_TAG}
  needs:
    - create_temporary_tags
  variables:
    PIPELINE_TOKEN:
    PROJECT_ID:
  script:
    - trigger --verbose --api-token "${API_TOKEN}" --pipeline-token "${PIPELINE_TOKEN}" --target-ref ${CI_PIPELINE_ID} --host "${CI_SERVER_HOST}" ${PROJECT_ID}
  tags:
    - public-runner-docker

hafah-build:
  extends: .external-build
  variables:
    PIPELINE_TOKEN: ${HAFAH_PIPELINE_TOKEN}
    PROJECT_ID: ${HAFAH_PROJECT_ID}

hivemind-build:
  extends: .external-build
  variables:
    PIPELINE_TOKEN: ${HIVEMIND_PIPELINE_TOKEN}
    PROJECT_ID: ${HIVEMIND_PROJECT_ID}

haf-api-node-build:
  extends: .external-build
  variables:
    PIPELINE_TOKEN: ${HAF_API_NODE_PIPELINE_TOKEN}
    PROJECT_ID: ${HAF_API_NODE_PROJECT_ID}

reputation-tracker-build:
  extends: .external-build
  variables:
    PIPELINE_TOKEN: ${REPUTATION_TRACKER_PIPELINE_TOKEN}
    PROJECT_ID: ${REPUTATION_TRACKER_PROJECT_ID}

remove_temporary_tags:
  extends: .job-defaults
  stage: build
  needs:
    - hafah-build
    - hivemind-build
    - haf-api-node-build
  image: 'registry.gitlab.syncad.com/hive/hive/ci-base-image${TEST_IMAGE_TAG}'
  before_script:
    - !reference [create_temporary_tags, before_script]
  script:
    - |
      set -e

      pushd hafah
      git push origin :refs/tags/${CI_PIPELINE_ID}
      popd

      pushd hivemind
      git push origin :refs/tags/${CI_PIPELINE_ID}
      popd

      pushd haf_api_node
      git push origin :refs/tags/${CI_PIPELINE_ID}
      popd

      pushd reptracker
      git push origin :refs/tags/${CI_PIPELINE_ID}
      popd
  tags:
    - public-runner-docker

mirrornet-haf-image-docker-build:
  stage: build
  extends: .prepare_haf_image
  variables:
    SUBMODULE_DIR: '${CI_PROJECT_DIR}/haf'
    REGISTRY_USER: '${HAF_REGISTRY_USER}'
    REGISTRY_PASS: '${HAF_REGISTRY_PASSWORD}'
    BINARY_CACHE_PATH: '${CI_PROJECT_DIR}/hived-mirrornet-binaries'
    HIVE_NETWORK_TYPE: mirrornet
    GIT_SUBMODULE_STRATEGY: recursive
  tags:
    - public-runner-docker
    - hived-for-tests

docker-build-auth:
  extends: .docker_build_template
  variables:
    TURBO_APP_SCOPE: '@hive/auth'
    TURBO_APP_PATH: '/apps/auth'
    TURBO_APP_NAME: 'auth'

docker-build-blog:
  extends: .docker_build_template
  variables:
    TURBO_APP_SCOPE: '@hive/blog'
    TURBO_APP_PATH: '/apps/blog'
    TURBO_APP_NAME: 'blog'

docker-build-wallet:
  extends: .docker_build_template
  variables:
    TURBO_APP_SCOPE: '@hive/wallet'
    TURBO_APP_PATH: '/apps/wallet'
    TURBO_APP_NAME: 'wallet'

# Hived binary built as a part of HAF requires an additional
# library to function. This library has to be extracted from
# the image and passed to job extended_block_log_creation.
library_extraction:
  extends: .job-defaults
  stage: build
  image:
    name: $HAF_IMAGE_NAME
    entrypoint: ['']
  needs:
    - job: 'mirrornet-haf-image-docker-build'
      artifacts: true
  variables:
    BINARY_CACHE_PATH: '${CI_PROJECT_DIR}/hived-mirrornet-binaries'
  script:
    - |
      set -e
      LIBPQ="$(sudo find / -name libpq.so.5* -type f 2>/dev/null)"
      echo "LIBPQ=${LIBPQ}"
      cp "${LIBPQ}" "${BINARY_CACHE_PATH}/libpq.so.5"
  artifacts:
    when: on_success
    paths:
      - '${BINARY_CACHE_PATH}'
  tags:
    - public-runner-docker

extended_block_log_creation:
  extends: .extended_block_log_creation
  stage: build
  needs:
    - job: 'mirrornet-haf-image-docker-build'
      artifacts: true
    - job: 'library_extraction'
      artifacts: true
  variables:
    GIT_SUBMODULE_STRATEGY: 'recursive'
    REGISTRY: 'registry.gitlab.syncad.com/hive/hive'
    REGISTRY_USER: '${HIVED_CI_IMGBUILDER_USER}'
    REGISTRY_PASS: '${HIVED_CI_IMGBUILDER_PASSWORD}'
    IMAGE_TAG: '${BLOCK_LOG_VERSION}'
    HIVE_SRC: '${CI_PROJECT_DIR}/haf/hive'
    BLOCK_LOG_SOURCE_DIR: '/blockchain/block_log_5m'
    BINARY_PATH: '${CI_PROJECT_DIR}/hived-mirrornet-binaries'
    LD_LIBRARY_PATH: '${BINARY_PATH}'
  tags:
    - public-runner-docker
    - hived-for-tests

block-log-extraction:
  extends: .docker_image_builder_job_template
  stage: replay-and-sync
  needs:
    - extended_block_log_creation
  variables:
    GIT_SUBMODULE_STRATEGY: 'recursive'
    BLOCK_LOG_DIRECTORY: '${REPLAY_PIPELINE_DIRECTORY}_block_log'
  script:
    - |
      set -e

      haf/scripts/ci-helpers/export-data-from-docker-image.sh \
        "${EXTENDED_BLOCK_LOG_IMAGE}" \
        "${BLOCK_LOG_DIRECTORY}" \
        --image-path=/blockchain/
      sudo chown -R hived:users "${BLOCK_LOG_DIRECTORY}"
      sudo chmod 777 "${BLOCK_LOG_DIRECTORY}"
      sudo chmod -R g+w "${BLOCK_LOG_DIRECTORY}"
      ls -lah "${BLOCK_LOG_DIRECTORY}"
  tags:
    - data-cache-storage

block_log_processing:
  extends: .job-defaults
  image: 'registry.gitlab.syncad.com/hive/hive/ci-base-image${TEST_IMAGE_TAG}'
  stage: replay-and-sync
  needs:
    - mirrornet-haf-image-docker-build
    - block-log-extraction
  variables:
    BLOCK_LOG_UTIL_PATH: '${CI_PROJECT_DIR}/hived-mirrornet-binaries/block_log_util'
  script:
    - |
      set -e

      if [ ! -e "${BLOCK_LOG_SOURCE}/block_log.artifacts" ]
      then
          echo "Generating blog_log.artifacts file..."
          "${BLOCK_LOG_UTIL_PATH}" --generate-artifacts --block-log "${BLOCK_LOG_SOURCE}/block_log"
      fi

      echo "Getting the head block's number..."
      HEAD_BLOCK_NUMBER=$("${BLOCK_LOG_UTIL_PATH}" --get-head-block-number --block-log "${BLOCK_LOG_SOURCE}/block_log")
      echo "Head block number is: ${HEAD_BLOCK_NUMBER}"

      echo "Obtaining timestamp of the latest block..."
      HEAD_BLOCK_TIMESTAMP=$("${BLOCK_LOG_UTIL_PATH}" --get-block -n "${HEAD_BLOCK_NUMBER}" --block-log "${BLOCK_LOG_SOURCE}/block_log" |  grep -o '"timestamp":"[^"]*"' | awk -F'"' '{print $4}' | sed 's/\(.*\)-\(.*\)-\(.*\)T\(.*\)/@\1-\2-\3 \4/')

      echo "Faketime will be set to ${HEAD_BLOCK_TIMESTAMP}"

      {
        echo "LAST_BLOCK_NUMBER=${HEAD_BLOCK_NUMBER}"
        echo "HEAD_BLOCK_TIMESTAMP=${HEAD_BLOCK_TIMESTAMP}"
      } > block_log_data.env
  artifacts:
    reports:
      dotenv:
        - 'block_log_data.env'
    paths:
      - '*.log'
      - 'block_log_data.env'
  tags:
    - data-cache-storage

mirrornet-haf-node-replay:
  extends: .haf-node-replay
  stage: replay-and-sync
  timeout: 3 hours
  needs:
    - mirrornet-haf-image-docker-build
    - hafah-build
    - hivemind-build
    - haf-api-node-build
    - block_log_processing
  variables:
    GIT_STRATEGY: 'clone'
    CI_DEBUG_SERVICES: 'false' # Change to true to debug services in this job
    HAF_VERSION: '${HAF_REGISTRY_TAG}'
    HAF_REGISTRY: '${HAF_REGISTRY_PATH}'
    # LAST_BLOCK_NUMBER: "5004741"
    ARGUMENTS: '--chain-id=44 --skeleton-key=5JNHfZYKGaomSFvd4NUdQ9qMcEAC43kujbfjueTHpVapX1Kzq2n --replay-blockchain --stop-at-block ${LAST_BLOCK_NUMBER} --alternate-chain-spec=/home/hived/datadir/blockchain/alternate-chain-spec.json'
    # FAKETIME: "@2016-09-16 01:39:06"
    FAKETIME: ${HEAD_BLOCK_TIMESTAMP}
    ADDITIONAL_CONFIGURATION_SCRIPT: 'scripts/ci-helpers/copy-mirrornet-haf-config.sh'
    REPLAY_TIMEOUT: 9000
  tags:
    - data-cache-storage

.node-job:
  extends: .job-defaults
  image:
    name: node:20.11.1-alpine3.18
    entrypoint: []
  stage: test
  before_script:
    - corepack enable
    - corepack prepare pnpm@latest-9 --activate
    - pnpm config set store-dir .pnpm-store
  script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):deps[collapsed=true]\r\e[0KInstalling dependencies..."
      pnpm install --frozen-lockfile
      echo -e "\e[0Ksection_end:$(date +%s):deps\r\e[0K"
  cache:
    key: '$TURBO_APP_SCOPE-cache-2'
    paths:
      - .npm/
      - .pnpm-store/
      - .next/
  artifacts:
    name: '$CI_JOB_NAME-$CI_COMMIT_REF_NAME'
    when: always
    expire_in: 1 week
  tags:
    - public-runner-docker

.e2e_tests_template:
  extends: .node-job
  image: mcr.microsoft.com/playwright:${PLAYWRIGHT_TAG}
  variables:
    CI_DEBUG_SERVICES: true
    FF_NETWORK_PER_BUILD: 1
    REACT_APP_API_ENDPOINT: https://api.hive.blog
    REACT_APP_IMAGES_ENDPOINT: https://images.hive.blog/
    DENSER_URL: https://caddy
  parallel:
    matrix:
      - PROJECT: ['chromium', 'firefox', 'webkit']
        SHARD_INDEX: [1, 2, 3, 4, 5]
        SHARD_TOTAL: 5
  script:
    - !reference [.node-job, script]
    - |
      echo -e "\e[0Ksection_start:$(date +%s):tests[collapsed=false]\r\e[0KRunning tests..."
      cd .$TURBO_APP_PATH
      npx playwright test --project=$PROJECT --shard=$SHARD_INDEX/$SHARD_TOTAL --update-snapshots
      echo -e "\e[0Ksection_end:$(date +%s):tests\r\e[0K"
  artifacts:
    paths:
      - .${TURBO_APP_PATH}/playwright-report/
      - .${TURBO_APP_PATH}/test-results/
      - .${TURBO_APP_PATH}/junit/
    reports:
      junit: .${TURBO_APP_PATH}/junit/**/**/results.xml

mirrornet-replay-data-copy:
  extends: .haf_api_node_replay_data_copy
  stage: test
  needs:
    - mirrornet-haf-node-replay
  tags:
    - data-cache-storage

mirrornet-based-tests:
  extends:
    - .haf_api_node_test
    - .node-job
  needs:
    - mirrornet-haf-image-docker-build
    - docker-build-auth
    - docker-build-blog
    - docker-build-wallet
    - mirrornet-replay-data-copy
    - block_log_processing
  image: 'mcr.microsoft.com/playwright:${PLAYWRIGHT_TAG}'
  services:
    - !reference [.haf_api_node_test, services]
    - name: $BLOG_IMAGE_NAME
      alias: denser-blog
      variables:
        HEALTHCHECK_TCP_PORT: '3000'
        PORT: '3000'
        REACT_APP_SITE_DOMAIN: 'https://caddy-blog.local'
        REACT_APP_WALLET_ENDPOINT: 'https://caddy-wallet'
        TURBO_APP_SCOPE: '@hive/blog'
        TURBO_APP_PATH: '/apps/blog'
    - name: caddy:${CADDY_TAG}
      alias: caddy-blog
      command:
        - caddy
        - reverse-proxy
        - --from=https://caddy-blog
        - --to=denser-blog:3000
        - --internal-certs
        # - --debug
        # - --access-log
    - name: $WALLET_IMAGE_NAME
      alias: denser-wallet
      variables:
        HEALTHCHECK_TCP_PORT: '4000'
        PORT: '4000'
        REACT_APP_SITE_DOMAIN: 'https://caddy-wallet.local'
        REACT_APP_BLOG_DOMAIN: 'https://caddy-blog'
        TURBO_APP_SCOPE: '@hive/wallet'
        TURBO_APP_PATH: '/apps/wallet'
    - name: caddy:${CADDY_TAG}
      alias: caddy-wallet
      command:
        - caddy
        - reverse-proxy
        - --from=https://caddy-wallet
        - --to=denser-wallet:4000
        - --internal-certs
        # - --debug
        # - --access-log
    - name: $AUTH_IMAGE_NAME
      alias: denser-auth
      variables:
        HEALTHCHECK_TCP_PORT: '5000'
        PORT: '5000'
        REACT_APP_SITE_DOMAIN: 'https://caddy-auth.local'
        TURBO_APP_SCOPE: '@hive/auth'
        TURBO_APP_PATH: '/apps/auth'
    - name: caddy:${CADDY_TAG}
      alias: caddy-auth
      command:
        - caddy
        - reverse-proxy
        - --from=https://caddy-auth
        - --to=denser-auth:5000
        - --internal-certs
        # - --debug
        # - --access-log
  variables:
    GIT_STRATEGY: 'clone'
    CI_DEBUG_SERVICES: 'false'
    HAF_VERSION: '${HAF_REGISTRY_TAG}'
    HAF_REGISTRY: '${HAF_REGISTRY_PATH}'
    ARGUMENTS: '--chain-id=44 --skeleton-key=5JNHfZYKGaomSFvd4NUdQ9qMcEAC43kujbfjueTHpVapX1Kzq2n --alternate-chain-spec=/home/hived/datadir/blockchain/alternate-chain-spec.json'
    # FAKETIME: "@2016-09-16 01:39:06"
    FAKETIME: ${HEAD_BLOCK_TIMESTAMP}
    REACT_APP_API_ENDPOINT: 'https://dind/'
    REACT_APP_IMAGES_ENDPOINT: 'https://images.hive.blog/'
    REACT_APP_CHAIN_ID: '44'
    REACT_APP_LOGGING_BROWSER_ENABLED: 'true'
    NODE_TLS_REJECT_UNAUTHORIZED: 0
  timeout: 3 hours
  script:
    - |
      set -e
      export
      "${CI_PROJECT_DIR}/scripts/ci-helpers/run-mirrornet-tests.sh"
  after_script:
    - |
      cp "${REPLAY_PIPELINE_DIRECTORY}/docker_entrypoint.log" "${CI_PROJECT_DIR}/haf.log"
      cp --recursive "${REPLAY_PIPELINE_DIRECTORY}/logs" "${CI_PROJECT_DIR}/logs"
  cache:
    key: 'mirrornet-tests-1'
  artifacts:
    paths:
      - '*.log'
      - 'logs/'
      - '*.env'
      - 'apps/blog/playwright-report/'
      - 'apps/blog/test-results/'
      - 'apps/blog/junit/'
  tags:
    - data-cache-storage

e2e-tests-blog:
  extends: .e2e_tests_template
  needs:
    - docker-build-blog
  services:
    - name: $BLOG_IMAGE_NAME
      alias: denser
      variables:
        HEALTHCHECK_TCP_PORT: '3000'
    - name: caddy:${CADDY_TAG}
      command:
        - caddy
        - reverse-proxy
        - --from=https://caddy
        - --to=denser:3000
        - --internal-certs
  variables:
    TURBO_APP_SCOPE: '@hive/blog'
    TURBO_APP_PATH: '/apps/blog'
    PORT: 3000

e2e-tests-wallet:
  extends: .e2e_tests_template
  needs:
    - docker-build-wallet
  services:
    - name: $WALLET_IMAGE_NAME
      alias: denser
      variables:
        HEALTHCHECK_TCP_PORT: '4000'
    - name: caddy:${CADDY_TAG}
      command:
        - caddy
        - reverse-proxy
        - --from=https://caddy
        - --to=denser:4000
        - --internal-certs
  variables:
    TURBO_APP_SCOPE: '@hive/wallet'
    TURBO_APP_PATH: '/apps/wallet'
    PORT: 4000

.e2e_report_aggregate_template:
  extends: .node-job
  stage: test-report-aggregate
  when: always
  image: mcr.microsoft.com/playwright:${PLAYWRIGHT_TAG}
  script:
    - cd .$TURBO_APP_PATH
    - pnpm install -D playwright-merge-html-reports
    - ./node_modules/.bin/playwright test tests/merge-reports --config playwright.merge.config.ts
  artifacts:
    paths:
      - .${TURBO_APP_PATH}/merged-html-report/
      - .${TURBO_APP_PATH}/junit/

e2e-report-aggregate-blog:
  extends: .e2e_report_aggregate_template
  needs:
    - e2e-tests-blog
  variables:
    TURBO_APP_SCOPE: '@hive/blog'
    TURBO_APP_PATH: '/apps/blog'
    PROJECTS: '["chromium", "firefox", "webkit"]'
    SHARD_TOTAL: 5

e2e-report-aggregate-wallet:
  extends: .e2e_report_aggregate_template
  needs:
    - e2e-tests-wallet
  variables:
    TURBO_APP_SCOPE: '@hive/wallet'
    TURBO_APP_PATH: '/apps/wallet'
    PROJECTS: '["chromium", "firefox", "webkit"]'
    SHARD_TOTAL: 5

publish:
  image:
    name: node:20.11.1-alpine3.18
    entrypoint: ['']
  extends: .job-defaults
  stage: deploy
  script:
    # Extract a few values from package.json
    - NPM_PACKAGE_NAME=$(node -p "require('./package.json').name")
    - NPM_PACKAGE_VERSION=$(node -p "require('./package.json').version")

    # Validate that the package name is properly scoped to the project's root namespace.
    # For more information, see https://docs.gitlab.com/ee/user/packages/npm_registry/#package-naming-convention
    - |
      if [[ ! $NPM_PACKAGE_NAME =~ ^@$CI_PROJECT_ROOT_NAMESPACE/ ]]; then
        echo "Invalid package scope! Packages must be scoped in the root namespace of the project, e.g. \"@${CI_PROJECT_ROOT_NAMESPACE}/${CI_PROJECT_NAME}\""
        echo 'For more information, see https://docs.gitlab.com/ee/user/packages/npm_registry/#package-naming-convention'
        exit 1
      fi

    # Compare the version in package.json to all published versions.
    # If the package.json version has not yet been published, run `npm publish`.
    - |
      if [[ "$(npm view ${NPM_PACKAGE_NAME} versions)" != *"'${NPM_PACKAGE_VERSION}'"* ]]; then
        npm publish
        echo "Successfully published version ${NPM_PACKAGE_VERSION} of ${NPM_PACKAGE_NAME} to GitLab's NPM registry: ${CI_PROJECT_URL}/-/packages"
      else
        echo "Version ${NPM_PACKAGE_VERSION} of ${NPM_PACKAGE_NAME} has already been published, so no new version has been published."
      fi
  rules:
    - if: '$CI_COMMIT_TAG && $CI_COMMIT_REF_PROTECTED == "true"'
      when: manual
      allow_failure: true
  tags:
    - public-runner-docker

.deploy_template_for_staging:
  extends: .job-defaults
  stage: deploy
  before_script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
      docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
      echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
  script:
    - |
      scripts/run_instance.sh \
        --image="$IMAGE_NAME" \
        --app-scope="$TURBO_APP_SCOPE" \
        --app-path="$TURBO_APP_PATH" \
        --api-endpoint="$API_ENDPOINT" \
        --chain-id="$CHAIN_ID" \
        --images-endpoint="$IMAGES_ENDPOINT" \
        --name="$CONTAINER_NAME" \
        --port=$PORT \
        --detach
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: manual
      allow_failure: true
  tags:
    - hs-denser

staging:deploy-auth:
  extends: .deploy_template_for_staging
  needs:
    - docker-build-auth
  variables:
    PORT: 7000
    API_ENDPOINT: https://api.hive.blog
    IMAGES_ENDPOINT: https://images.hive.blog/
    CONTAINER_NAME: denser-auth
    TURBO_APP_SCOPE: '@hive/auth'
    TURBO_APP_PATH: '/apps/auth'
  environment:
    name: staging-auth
    action: start
    on_stop: staging:stop-auth

staging:deploy-blog:
  extends: .deploy_template_for_staging
  needs:
    - docker-build-blog
  variables:
    PORT: 3000
    API_ENDPOINT: https://api.hive.blog
    IMAGES_ENDPOINT: https://images.hive.blog/
    WALLET_ENDPOINT: https://wallet.openhive.network
    CONTAINER_NAME: denser-blog
    TURBO_APP_SCOPE: '@hive/blog'
    TURBO_APP_PATH: '/apps/blog'
  environment:
    name: staging-blog
    action: start
    on_stop: staging:stop-blog

staging:deploy-wallet:
  extends: .deploy_template_for_staging
  needs:
    - docker-build-wallet
  variables:
    PORT: 4000
    API_ENDPOINT: https://api.hive.blog
    IMAGES_ENDPOINT: https://images.hive.blog/
    CONTAINER_NAME: denser-wallet
    TURBO_APP_SCOPE: '@hive/wallet'
    TURBO_APP_PATH: '/apps/wallet'
  environment:
    name: staging-wallet
    action: start
    on_stop: staging:stop-wallet

.stop_template_for_staging:
  extends: .job-defaults
  stage: deploy
  script:
    - docker ps -q --filter "name=$CONTAINER_NAME" | grep -q . && docker stop $CONTAINER_NAME
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH
      when: manual
      allow_failure: true
  tags:
    - hs-denser

staging:stop-auth:
  extends: .stop_template_for_staging
  needs:
    - staging:deploy-auth
  environment:
    name: staging-auth
    action: stop
  variables:
    CONTAINER_NAME: denser-auth
    GIT_STRATEGY: none

staging:stop-blog:
  extends: .stop_template_for_staging
  needs:
    - staging:deploy-blog
  environment:
    name: staging-blog
    action: stop
  variables:
    CONTAINER_NAME: denser-blog
    GIT_STRATEGY: none

staging:stop-wallet:
  extends: .stop_template_for_staging
  needs:
    - staging:deploy-wallet
  environment:
    name: staging-wallet
    action: stop
  variables:
    CONTAINER_NAME: denser-wallet
    GIT_STRATEGY: none

review:deploy-auth:
  extends: staging:deploy-auth
  variables:
    CONTAINER_NAME: denser-review-auth
    PORT: 7001
    AUTH_PORT: 7001
    API_ENDPOINT: https://api.fake.openhive.network/
    CHAIN_ID: '42'
    IMAGES_ENDPOINT: https://images.hive.blog/
    SITE_DOMAIN: https://auth.openhive.network
    LOGGING_BROWSER_ENABLED: false
    LOGGING_LOG_LEVEL: info
  environment:
    name: review-auth
    action: start
    on_stop: review:stop-auth

review:deploy-blog:
  extends: staging:deploy-blog
  variables:
    CONTAINER_NAME: denser-review-blog
    PORT: 3001
    BLOG_PORT: 3001
    API_ENDPOINT: https://api.fake.openhive.network/
    CHAIN_ID: '42'
    IMAGES_ENDPOINT: https://images.hive.blog/
    WALLET_ENDPOINT: https://wallet.openhive.network
    SITE_DOMAIN: https://blog.openhive.network
    LOGGING_BROWSER_ENABLED: false
    LOGGING_LOG_LEVEL: info
  environment:
    name: review-blog
    action: start
    on_stop: review:stop-blog

review:deploy-wallet:
  extends: staging:deploy-wallet
  variables:
    CONTAINER_NAME: denser-review-wallet
    PORT: 4001
    WALLET_PORT: 4001
    API_ENDPOINT: https://api.fake.openhive.network/
    CHAIN_ID: '42'
    IMAGES_ENDPOINT: https://images.hive.blog/
    SITE_DOMAIN: https://wallet.openhive.network
    BLOG_DOMAIN: https://blog.openhive.network
    LOGGING_BROWSER_ENABLED: false
    LOGGING_LOG_LEVEL: info
  environment:
    name: review-wallet
    action: start
    on_stop: review:stop-wallet

review:stop-auth:
  extends: staging:stop-auth
  needs:
    - review:deploy-auth
  variables:
    CONTAINER_NAME: denser-review-auth
  environment:
    name: review-auth
    action: stop

review:stop-blog:
  extends: staging:stop-blog
  needs:
    - review:deploy-blog
  variables:
    CONTAINER_NAME: denser-review-blog
  environment:
    name: review-blog
    action: stop

review:stop-wallet:
  extends: staging:stop-wallet
  needs:
    - review:deploy-wallet
  variables:
    CONTAINER_NAME: denser-review-wallet
  environment:
    name: review-wallet
    action: stop

build_and_publish_auth_image:
  stage: publish
  extends: .publish_docker_image_template
  variables:
    TURBO_APP_SCOPE: '@hive/auth'
    TURBO_APP_PATH: '/apps/auth'
    TURBO_APP_NAME: 'auth'
  before_script:
    - !reference [.publish_docker_image_template, before_script]
  script:
    - scripts/ci-helpers/build_and_publish_instance.sh
  tags:
    - public-runner-docker
    - hived-for-tests

build_and_publish_blog_image:
  stage: publish
  extends: .publish_docker_image_template
  variables:
    TURBO_APP_SCOPE: '@hive/blog'
    TURBO_APP_PATH: '/apps/blog'
    TURBO_APP_NAME: 'blog'
  before_script:
    - !reference [.publish_docker_image_template, before_script]
  script:
    - scripts/ci-helpers/build_and_publish_instance.sh
  tags:
    - public-runner-docker
    - hived-for-tests

build_and_publish_wallet_image:
  stage: publish
  extends: .publish_docker_image_template
  variables:
    TURBO_APP_SCOPE: '@hive/wallet'
    TURBO_APP_PATH: '/apps/wallet'
    TURBO_APP_NAME: 'wallet'
  before_script:
    - !reference [.publish_docker_image_template, before_script]
  script:
    - scripts/ci-helpers/build_and_publish_instance.sh
  tags:
    - public-runner-docker
    - hived-for-tests

buildkit_cache_cleanup:
  stage: cleanup
  extends: .buildkit_cleanup_job_template
  needs: []
  variables:
    CACHE_REPOSITORIES: 'auth/cache,blog/cache,wallet/cache'

# Deletes replay data used by the tests and created by mirrornet-replay-data-copy
cleanup_haf_api_node_pipeline_cache:
  needs:
    - mirrornet-replay-data-copy
    - mirrornet-based-tests
  extends:
    - .cleanup_cache_manual_template
  stage: cleanup
  variables:
    CLEANUP_PATH_PATTERN: '${REPLAY_PIPELINE_DIRECTORY}*'
  when: always
  tags:
    - data-cache-storage

# Deletes all HAF API node replay data
cleanup_haf_api_node_cache_manual:
  extends:
    - .cleanup_cache_manual_template
  needs: []
  stage: cleanup
  variables:
    CLEANUP_PATH_PATTERN: '${REPLAY_DIRECTORY_PREFIX}*'
  tags:
    - data-cache-storage

# Deletes HAF API node replay data older than 7 days
cleanup_old_haf_api_node_cache:
  extends:
    - .cleanup_old_cache_template
  needs: []
  stage: cleanup
  variables:
    CLEANUP_PATH_PATTERN: '${REPLAY_DIRECTORY_PREFIX}*'
  tags:
    - data-cache-storage
